{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - an Introduction-cum-Intution\n",
    "![](img/NNIntroImage.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all have brains and that contains neuron cells look somewhat like this,\n",
    "![](img/BiologicalNeuron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their prime work is to process and transfer information and we too this characterstic of neurons in our neural network algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we do in a Neural Network(In a Nutshell)\n",
    "We process and transfer information and that information is data, humongous amount of data, which gets projected into higher dimension and then gets transferred to another neuron for projection on more higher dimension and then in the end we calculate the error that is to be minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Neural Network vs. Linear/Logistic Regression\n",
    "\n",
    "#### Let's take a quick recap of what we do in Linear Regression Algorithm\n",
    "We have to fit a line in linear regression algorithm inbetween the dataset so that we can predict anytime.\n",
    "That line sort of generalizes the data for predictions for some other data points.\n",
    "So in a nutshell we find a pattern using that line.\n",
    "\n",
    "#### But in Neural Network that line is replaced by some hyperplanes in higher dimensions we can't imagine and we try to fit those hyperplanes/curves in between the data in such a way that gives us accurate predictions/classifications.\n",
    "And those higher dimensional hyperplanes/curves are made by our Activation functions such as ReLu, Leaky ReLu, Sigmoid, HyperbolicTan(Tanh), Linear, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers:\n",
    "\n",
    "### A Neural Network consists of Layers which contains neurons which look somewhat like this:\n",
    "![](img/NNLayerAndNeuron.jpg)\n",
    "\n",
    "#### 1. Input Layer:\n",
    "This input layer is the entry point of Neural Network which contains Neurons equal to No. of Features in our Dataset. Neurons of this layer only has output, no input.\n",
    "\n",
    "#### 2. Hidden Layer(s):\n",
    "Hidden layer could be one or more than one in number and all the magic happens here, these hidden layers project the data into higher dimension and make new features out of elementary features from input layer. Neurons of these layers have inputs as well as outputs.\n",
    "\n",
    "#### 3. Output Layer:\n",
    "This layer contains no of neurons equal to classes when we are doing classification and a single neuron when we are doing prediction.This layer scales down all the high dimensional data according to our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons:\n",
    "### Each layers contain Neurons and those neurons contains weights(except input layer) which gets multiplied by our data and then gets out after processing under an Activation Function, this neuron looks somewhat like this\n",
    "\n",
    "![](img/NeuronBasicThetasInputsAndOutputs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a neuron gets inputs, a specific weight gets initialized for each input and these weights are denoted by Thetas which have got a special way of expressing their origin, notice the numbers written in Superscript and Subscript of Thetas, each of those numbers denote something special. Let's see what,\n",
    "![](img/ThetaNotation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Outputs of Neuron\n",
    "Inputs for first neuron are respective features of data let's see what happens with those inputs and data and how outputs form.\n",
    "![](img/ProcessingOfInputsAndOutputs.jpg)\n",
    "### Note: A Neuron Emits same output for all the output wires \n",
    "Then the output from previous neuron becomes input for next neuron present in next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it becomes a series in which first it starts with Dataset and then Activation function from first neuron converts it to input for next layer neurons and it goes on until last layer which is output layer.\n",
    "And at the output layer we get the most important thing for training, the error function which is to be minimized using Optimization algorithm.\n",
    "\n",
    "This error function is calculated by subtracting the predictions by actual class or prices or whatever thing your dataset has got.\n",
    "And this is what basic introduction of Neural Network is. The Neural Network we had studied is called Forward Pass Neural Network and there are some other types of Neural Networks also which we will be discussing in further notebooks, such as,\n",
    "\n",
    "1. Convolutional Neural Network\n",
    "2. Recurrent Neural Network\n",
    "3. Generative Adversial Networks\n",
    "4. Self Organising Maps\n",
    "5. Restricted Boltzmann Machines\n",
    "\n",
    "All of these are based on Neural Network basic functionality and we will be discussing the changes as well as learning which network is optimum for certain use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Error function using Optimization Algorithm - Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First understand what is Gradient Descent:\n",
    "It is an optimization algorithm used to minimize an Error using a feasible way using Programming.\n",
    "\n",
    "### How to use it in Neural Networks\n",
    "In neural network we have got weights and whenever we used Gradient Descent, we subtracted something from weights and that something was partial derivative of cost function with respect to weight from which it is being subtracted.\n",
    "\n",
    "And in neural network we have got weights in each layer and we subtract gradient from each weight and that is Gradient Descent here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
